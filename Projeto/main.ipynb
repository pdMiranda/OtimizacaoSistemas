{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830caf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331a89d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Carregando dados\")\n",
    "data = np.loadtxt('./data/wine.data', delimiter=',')\n",
    "\n",
    "X, y = data[:, 1:], data[:,0]\n",
    "\n",
    "# Transforma em problema de classificação binária\n",
    "idxs = [i for i in range(len(y)) if y[i] == 1 or y[i] == 2]\n",
    "X, y = X[idxs], y[idxs]\n",
    "\n",
    "# Normaliza os dados\n",
    "X = (X - X.mean(axis=0))/(X.max(axis=0) - X.min(axis=0))\n",
    "X = np.hstack((X, np.ones(len(X)).reshape(len(X),1)))\n",
    "\n",
    "# Transforma variável target\n",
    "y = np.array(list(map(lambda x: 0 if x == 1 else 1, y)))\n",
    "\n",
    "print(f\"Dataset: {X.shape[0]} amostras, {X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTreinando modelo benchmark (Scikit-Learn)\")\n",
    "reg = LogisticRegression(solver='sag', C=100000, max_iter=10000).fit(X, y)\n",
    "L_star = log_loss(y, reg.predict_proba(X))\n",
    "print(f\"Loss L* (benchmark) = {L_star:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4777d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def cross_entropy_loss(y, y_pred):\n",
    "    epsilon = 1e-15  # Para evitar log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    loss = -(1/len(y))*np.sum(y*np.log(y_pred) + (1 - y)*np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_grad(y, y_pred, X):\n",
    "    return list(np.dot((y_pred - y), X)[0])\n",
    "\n",
    "def compute_loss_for_weights(w, X, y):\n",
    "    \"\"\"Calcula loss para um dado conjunto de pesos\"\"\"\n",
    "    y_pred = sigmoid(np.dot(w.T, X.T))\n",
    "    return cross_entropy_loss(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c509263",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DESCIDA COORDENADA COM GRADIENTE (14 PESOS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "w_gradient = np.zeros(14).reshape(14, 1)\n",
    "eta = 0.01\n",
    "num_features = 14\n",
    "\n",
    "# Parâmetros de convergência\n",
    "patience = 1000  # Número de iterações sem melhoria significativa\n",
    "min_improvement = 1e-6  # Melhoria mínima considerada significativa\n",
    "max_iter = 1000000  # Limite máximo de iterações\n",
    "\n",
    "loss_gradient = []\n",
    "weights_history_gradient = []\n",
    "gradient_selections = np.zeros(num_features)\n",
    "iteration_log = []  # Para tabela de evolução\n",
    "\n",
    "best_loss = float('inf')\n",
    "no_improvement_count = 0\n",
    "\n",
    "print(f\"\\nParâmetros de convergência:\")\n",
    "print(f\"  - Paciência: {patience} iterações\")\n",
    "print(f\"  - Melhoria mínima: {min_improvement}\")\n",
    "print(f\"  - Máximo de iterações: {max_iter}\")\n",
    "\n",
    "start_time = time.time()\n",
    "pbar = tqdm(desc=\"Descida com Gradiente (14 pesos)\", total=max_iter)\n",
    "\n",
    "for t in range(max_iter):\n",
    "    y_pred = sigmoid(np.dot(w_gradient.T, X.T))\n",
    "    current_loss = cross_entropy_loss(y, y_pred)\n",
    "    loss_gradient.append(current_loss)\n",
    "    \n",
    "    grad = cross_entropy_grad(y, y_pred, X)\n",
    "    \n",
    "    # Seleciona a coordenada com maior gradiente (em valor absoluto)\n",
    "    grad_abs = [abs(g) for g in grad]\n",
    "    best_index = grad_abs.index(max(grad_abs))\n",
    "    gradient_selections[best_index] += 1\n",
    "    \n",
    "    # Atualiza apenas o peso com maior gradiente\n",
    "    w_gradient[best_index] = w_gradient[best_index] - eta*grad[best_index]\n",
    "    \n",
    "    # Salva histórico\n",
    "    weights_history_gradient.append(w_gradient.flatten().copy())\n",
    "    \n",
    "    # Log a cada 1000 iterações\n",
    "    if t % 1000 == 0 or t < 10:\n",
    "        iteration_log.append({\n",
    "            'Iteração': t,\n",
    "            'Loss': current_loss,\n",
    "            'Melhor Índice': best_index,\n",
    "            'Gradiente Max': max(grad_abs),\n",
    "            'Delta Loss': current_loss - best_loss if t > 0 else 0\n",
    "        })\n",
    "    \n",
    "    # Verifica convergência\n",
    "    if current_loss < best_loss - min_improvement:\n",
    "        best_loss = current_loss\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "    \n",
    "    # Critério de parada\n",
    "    if no_improvement_count >= patience:\n",
    "        print(f\"\\nConvergência atingida na iteração {t}\")\n",
    "        print(f\"  Sem melhoria significativa por {patience} iterações\")\n",
    "        pbar.close()\n",
    "        break\n",
    "    \n",
    "    pbar.update(1)\n",
    "    if t == max_iter - 1:\n",
    "        print(f\"\\nMáximo de iterações ({max_iter}) atingido\")\n",
    "        pbar.close()\n",
    "\n",
    "time_gradient = time.time() - start_time\n",
    "weights_history_gradient = np.array(weights_history_gradient)\n",
    "total_iterations_14 = len(loss_gradient)\n",
    "\n",
    "y_pred_gradient = sigmoid(np.dot(w_gradient.T, X.T))\n",
    "y_pred_gradient_class = np.array(list(map(lambda x: 1 if x >= 0.5 else 0, y_pred_gradient.flatten())))\n",
    "\n",
    "print(f\"\\nResultados:\")\n",
    "print(f\"  Tempo de execução: {time_gradient:.2f}s\")\n",
    "print(f\"  Total de iterações: {total_iterations_14:,}\")\n",
    "print(f\"  Acurácia: {accuracy_score(y, y_pred_gradient_class):.4f}\")\n",
    "print(f\"  Loss final: {loss_gradient[-1]:.10f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TABELA DE EVOLUÇÃO (Primeiras e últimas iterações)\")\n",
    "print(\"=\"*70)\n",
    "df_log = pd.DataFrame(iteration_log)\n",
    "print(\"\\nPrimeiras 5 iterações:\")\n",
    "print(df_log.head(5).to_string(index=False))\n",
    "print(\"\\nÚltimas iterações registradas:\")\n",
    "print(df_log.tail(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29380f27",
   "metadata": {},
   "source": [
    "CRITÉRIOS DE SELEÇÃO DOS 2 PESOS MAIS IMPORTANTES\n",
    "\n",
    "Frequência de Seleção: Quantas vezes o peso foi escolhido para atualização -> Indica quão frequentemente o peso precisa ser ajustado\n",
    "\n",
    "Magnitude Final: Valor absoluto do peso após convergência -> Indica o impacto direto do peso na predição\n",
    "\n",
    "Variância: Quanto o peso mudou durante o treinamento -> Indica a sensibilidade e importância do peso no processo de otimização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83303b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANÁLISE DE IMPORTÂNCIA DOS PESOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Critério 1: Frequência de seleção durante otimização\n",
    "freq_normalized = gradient_selections / gradient_selections.sum()\n",
    "\n",
    "# Critério 2: Magnitude do peso final\n",
    "weight_magnitude = np.abs(w_gradient.flatten())\n",
    "magnitude_normalized = weight_magnitude / weight_magnitude.sum()\n",
    "\n",
    "# Critério 3: Variância do peso durante treinamento\n",
    "weight_variance = np.var(weights_history_gradient, axis=0)\n",
    "variance_normalized = weight_variance / weight_variance.sum()\n",
    "\n",
    "# Score combinado (média dos 3 critérios)\n",
    "importance_score = (freq_normalized + magnitude_normalized + variance_normalized) / 3\n",
    "\n",
    "# Seleciona os 2 pesos mais importantes\n",
    "top_2_indices = np.argsort(importance_score)[-2:][::-1]\n",
    "\n",
    "print(f\"\\nTOP 2 PESOS MAIS IMPORTANTES:\")\n",
    "for rank, idx in enumerate(top_2_indices, 1):\n",
    "    print(f\"\\n#{rank} - Peso w_{idx}:\")\n",
    "    print(f\"  - Frequência de seleção: {gradient_selections[idx]:.0f} vezes ({freq_normalized[idx]*100:.2f}%)\")\n",
    "    print(f\"  - Magnitude final: {weight_magnitude[idx]:.6f} ({magnitude_normalized[idx]*100:.2f}%)\")\n",
    "    print(f\"  - Variância: {weight_variance[idx]:.6f} ({variance_normalized[idx]*100:.2f}%)\")\n",
    "    print(f\"  - Score de importância: {importance_score[idx]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14937bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RETREINAMENTO COM APENAS OS 2 PESOS MAIS IMPORTANTES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cria dataset reduzido com as 2 features mais importantes + BIAS\n",
    "# O bias (coluna de 1s) é sempre a última coluna (índice 13)\n",
    "if 13 in top_2_indices:\n",
    "    # Se o bias já está nos top 2, usa apenas eles\n",
    "    X_reduced = X[:, top_2_indices]\n",
    "    print(f\"Dataset reduzido: features {top_2_indices[0]} e {top_2_indices[1]} (inclui bias)\")\n",
    "else:\n",
    "    # Se o bias não está nos top 2, adiciona ele manualmente\n",
    "    feature_indices = list(top_2_indices) + [13]\n",
    "    X_reduced = X[:, feature_indices]\n",
    "    print(f\"Dataset reduzido: features {top_2_indices[0]}, {top_2_indices[1]} + bias (w_13)\")\n",
    "    \n",
    "num_weights_2d = X_reduced.shape[1]\n",
    "\n",
    "w_2d = np.zeros(num_weights_2d).reshape(num_weights_2d, 1)\n",
    "loss_2d = []\n",
    "weights_history_2d = []\n",
    "iteration_log_2d = []\n",
    "\n",
    "best_loss_2d = float('inf')\n",
    "no_improvement_count_2d = 0\n",
    "\n",
    "start_time = time.time()\n",
    "pbar = tqdm(desc=f\"Descida com Gradiente ({num_weights_2d} pesos)\", total=max_iter)\n",
    "\n",
    "for t in range(max_iter):\n",
    "    y_pred = sigmoid(np.dot(w_2d.T, X_reduced.T))\n",
    "    current_loss = cross_entropy_loss(y, y_pred)\n",
    "    loss_2d.append(current_loss)\n",
    "    \n",
    "    grad = list(np.dot((y_pred - y), X_reduced)[0])\n",
    "    grad_abs = [abs(g) for g in grad]\n",
    "    best_index = grad_abs.index(max(grad_abs))\n",
    "    \n",
    "    w_2d[best_index] = w_2d[best_index] - eta*grad[best_index]\n",
    "    weights_history_2d.append(w_2d.flatten().copy())\n",
    "    \n",
    "    # Log a cada 100 iterações (mais frequente para ver melhor)\n",
    "    if t % 100 == 0 or t < 10:\n",
    "        log_entry = {\n",
    "            'Iteracao': t,\n",
    "            'Loss': current_loss,\n",
    "            'Coord. Atualizada': f'w_{feature_indices[best_index] if num_weights_2d == 3 else top_2_indices[best_index]}',\n",
    "            'Delta Loss': current_loss - best_loss_2d if t > 0 else 0\n",
    "        }\n",
    "        # Adiciona os valores dos pesos\n",
    "        for i in range(num_weights_2d):\n",
    "            weight_name = f'w_{feature_indices[i]}' if num_weights_2d == 3 else f'w_{top_2_indices[i]}'\n",
    "            log_entry[weight_name] = w_2d[i, 0]\n",
    "        iteration_log_2d.append(log_entry)\n",
    "    \n",
    "    # Verifica convergência\n",
    "    if current_loss < best_loss_2d - min_improvement:\n",
    "        best_loss_2d = current_loss\n",
    "        no_improvement_count_2d = 0\n",
    "    else:\n",
    "        no_improvement_count_2d += 1\n",
    "    \n",
    "    # Critério de parada\n",
    "    if no_improvement_count_2d >= patience:\n",
    "        print(f\"\\nConvergência atingida na iteração {t}\")\n",
    "        print(f\"  Sem melhoria significativa por {patience} iterações\")\n",
    "        pbar.close()\n",
    "        break\n",
    "    \n",
    "    pbar.update(1)\n",
    "    if t == max_iter - 1:\n",
    "        print(f\"\\nMáximo de iterações ({max_iter}) atingido\")\n",
    "        pbar.close()\n",
    "\n",
    "time_2d = time.time() - start_time\n",
    "weights_history_2d = np.array(weights_history_2d)\n",
    "total_iterations_2 = len(loss_2d)\n",
    "\n",
    "y_pred_2d = sigmoid(np.dot(w_2d.T, X_reduced.T))\n",
    "y_pred_2d_class = np.array(list(map(lambda x: 1 if x >= 0.5 else 0, y_pred_2d.flatten())))\n",
    "\n",
    "print(f\"\\nResultados:\")\n",
    "print(f\"  Tempo de execucao: {time_2d:.2f}s\")\n",
    "print(f\"  Total de iteracoes: {total_iterations_2:,}\")\n",
    "print(f\"  Acuracia: {accuracy_score(y, y_pred_2d_class):.4f}\")\n",
    "print(f\"  Loss final: {loss_2d[-1]:.10f}\")\n",
    "print(f\"  Pesos utilizados: {num_weights_2d} ({top_2_indices[0]}, {top_2_indices[1]}\" + \n",
    "      (f\", 13 (bias))\" if num_weights_2d == 3 else \")\"))\n",
    "\n",
    "# Mostra tabela de evolução 2D\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"TABELA DE EVOLUCAO - {num_weights_2d} PESOS (Amostragem)\")\n",
    "print(\"=\"*70)\n",
    "df_log_2d = pd.DataFrame(iteration_log_2d)\n",
    "print(\"\\nPrimeiras 5 iteracoes:\")\n",
    "print(df_log_2d.head(5).to_string(index=False))\n",
    "print(\"\\nUltimas iteracoes registradas:\")\n",
    "print(df_log_2d.tail(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARAÇÃO FINAL DE RESULTADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nCOMPARACAO DE PERDA (LOSS):\")\n",
    "print(f\"  Benchmark (Scikit-Learn):         {L_star:.10f}\")\n",
    "print(f\"  Descida com Gradiente (14 pesos): {loss_gradient[-1]:.10f}  (diff: +{loss_gradient[-1] - L_star:.10f})\")\n",
    "print(f\"  Descida com Gradiente ({num_weights_2d} pesos):  {loss_2d[-1]:.10f}  (diff: +{loss_2d[-1] - L_star:.10f})\")\n",
    "\n",
    "print(\"\\nCOMPARACAO DE TEMPO:\")\n",
    "print(f\"  Descida com Gradiente (14 pesos): {time_gradient:.2f}s  ({total_iterations_14:,} iteracoes)\")\n",
    "print(f\"  Descida com Gradiente ({num_weights_2d} pesos):  {time_2d:.2f}s  ({total_iterations_2:,} iteracoes)\")\n",
    "print(f\"  Speedup: {time_gradient/time_2d:.2f}x mais rapido\")\n",
    "\n",
    "print(\"\\nEFICIENCIA (Loss vs Tempo):\")\n",
    "print(f\"  14 pesos: {loss_gradient[-1]:.6f} loss em {time_gradient:.2f}s\")\n",
    "print(f\"  {num_weights_2d} pesos:  {loss_2d[-1]:.6f} loss em {time_2d:.2f}s\")\n",
    "degradation = ((loss_2d[-1] - loss_gradient[-1])/loss_gradient[-1]*100)\n",
    "speedup_pct = ((time_gradient - time_2d)/time_gradient*100)\n",
    "print(f\"  Trade-off: {degradation:.2f}% mais loss para {speedup_pct:.2f}% menos tempo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbd4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparação de Loss ao longo das iterações\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(loss_gradient, 'r-', label='Descida com Gradiente (14 pesos)', alpha=0.7, linewidth=1.5)\n",
    "plt.plot(loss_2d, 'g-', label=f'Descida com Gradiente ({num_weights_2d} pesos)', alpha=0.7, linewidth=1.5)\n",
    "plt.axhline(y=L_star, color='orange', linestyle='--', linewidth=2, label='L* (Benchmark)')\n",
    "plt.title('Comparação: Iteração vs Loss', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Iteração', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.xlim(0, max(total_iterations_14, total_iterations_2))\n",
    "plt.ylim(-0.01, 0.15)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evolução dos 14 pesos (Descida com Gradiente)\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
    "fig.suptitle('Evolução de Cada Peso - Descida Coordenada com Gradiente (14 pesos)', \n",
    "             fontsize=24, fontweight='bold')\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for i in range(num_features):\n",
    "    ax = axes_flat[i]\n",
    "    color = 'red' if i in top_2_indices else 'steelblue'\n",
    "    linewidth = 2.5 if i in top_2_indices else 1.0\n",
    "    ax.plot(weights_history_gradient[:, i], color=color, linewidth=linewidth)\n",
    "    \n",
    "    title = f'Peso w_{i}'\n",
    "    if i in top_2_indices:\n",
    "        title += ' (TOP 2)'\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold' if i in top_2_indices else 'normal')\n",
    "    ax.set_xlabel('Iteração', fontsize=10)\n",
    "    ax.set_ylabel('Valor', fontsize=10)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "axes_flat[14].axis('off')\n",
    "axes_flat[15].axis('off')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# Criar grade de loss para visualizar espaço de soluções\n",
    "print(\"\\nCalculando espaco de solucoes\")\n",
    "\n",
    "# Encontra os limites da trajetória com margem\n",
    "w0_min, w0_max = weights_history_2d[:, 0].min(), weights_history_2d[:, 0].max()\n",
    "w1_min, w1_max = weights_history_2d[:, 1].min(), weights_history_2d[:, 1].max()\n",
    "\n",
    "# Adiciona margem de 50% em cada direção\n",
    "w0_margin = (w0_max - w0_min) * 0.5\n",
    "w1_margin = (w1_max - w1_min) * 0.5\n",
    "\n",
    "w0_range = np.linspace(w0_min - w0_margin, w0_max + w0_margin, 100)\n",
    "w1_range = np.linspace(w1_min - w1_margin, w1_max + w1_margin, 100)\n",
    "W0, W1 = np.meshgrid(w0_range, w1_range)\n",
    "\n",
    "# Calcula loss para cada combinação de pesos\n",
    "Z = np.zeros_like(W0)\n",
    "\n",
    "# Se temos 3 pesos (2 features + bias), precisamos fixar o bias\n",
    "if num_weights_2d == 3:\n",
    "    bias_value = w_2d[2, 0]  # Valor final do bias\n",
    "    for i in range(W0.shape[0]):\n",
    "        for j in range(W0.shape[1]):\n",
    "            w_temp = np.array([[W0[i, j]], [W1[i, j]], [bias_value]])\n",
    "            Z[i, j] = compute_loss_for_weights(w_temp, X_reduced, y)\n",
    "else:\n",
    "    for i in range(W0.shape[0]):\n",
    "        for j in range(W0.shape[1]):\n",
    "            w_temp = np.array([[W0[i, j]], [W1[i, j]]])\n",
    "            Z[i, j] = compute_loss_for_weights(w_temp, X_reduced, y)\n",
    "\n",
    "# Visualização 3D com superfície de loss\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Vista 3D com superfície\n",
    "ax1 = fig.add_subplot(221, projection='3d')\n",
    "surface = ax1.plot_surface(W0, W1, Z, cmap='viridis', alpha=0.6, edgecolor='none', linewidth=0, antialiased=True)\n",
    "iterations = np.arange(len(weights_history_2d))\n",
    "scatter = ax1.scatter(weights_history_2d[:, 0], \n",
    "                      weights_history_2d[:, 1], \n",
    "                      loss_2d,\n",
    "                      c=iterations, \n",
    "                      cmap='hot', \n",
    "                      s=3,\n",
    "                      alpha=0.8)\n",
    "ax1.plot(weights_history_2d[:, 0], \n",
    "         weights_history_2d[:, 1], \n",
    "         loss_2d, \n",
    "         'r-', \n",
    "         alpha=0.5, \n",
    "         linewidth=1.5,\n",
    "         label='Trajetoria')\n",
    "ax1.scatter([weights_history_2d[0, 0]], \n",
    "            [weights_history_2d[0, 1]], \n",
    "            [loss_2d[0]], \n",
    "            c='lime', \n",
    "            s=200, \n",
    "            marker='o', \n",
    "            label='Inicio',\n",
    "            edgecolors='black',\n",
    "            linewidths=2)\n",
    "ax1.scatter([weights_history_2d[-1, 0]], \n",
    "            [weights_history_2d[-1, 1]], \n",
    "            [loss_2d[-1]], \n",
    "            c='red', \n",
    "            s=200, \n",
    "            marker='*', \n",
    "            label='Final',\n",
    "            edgecolors='black',\n",
    "            linewidths=2)\n",
    "ax1.set_xlabel(f'w_{top_2_indices[0]}', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel(f'w_{top_2_indices[1]}', fontsize=12, fontweight='bold')\n",
    "ax1.set_zlabel('Loss', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Trajetoria 3D da Descida sobre Superficie de Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.view_init(elev=25, azim=45)\n",
    "\n",
    "# Projeção 2D com curvas de nível\n",
    "ax2 = fig.add_subplot(222)\n",
    "levels = 30\n",
    "contour = ax2.contour(W0, W1, Z, levels=levels, cmap='viridis', alpha=0.6, linewidths=0.5)\n",
    "ax2.clabel(contour, inline=True, fontsize=7, fmt='%.4f')\n",
    "contourf = ax2.contourf(W0, W1, Z, levels=levels, cmap='viridis', alpha=0.4)\n",
    "\n",
    "# Trajetória\n",
    "trajectory = ax2.plot(weights_history_2d[:, 0], \n",
    "                      weights_history_2d[:, 1], \n",
    "                      'r-', \n",
    "                      alpha=0.7, \n",
    "                      linewidth=2,\n",
    "                      label='Trajetoria')\n",
    "# Mostra pontos a cada 50 iterações para não poluir\n",
    "step = max(1, len(weights_history_2d) // 50)\n",
    "scatter2 = ax2.scatter(weights_history_2d[::step, 0], \n",
    "                       weights_history_2d[::step, 1], \n",
    "                       c=iterations[::step], \n",
    "                       cmap='hot', \n",
    "                       s=30, \n",
    "                       alpha=0.8,\n",
    "                       edgecolors='black',\n",
    "                       linewidths=0.5,\n",
    "                       zorder=5)\n",
    "ax2.scatter([weights_history_2d[0, 0]], \n",
    "            [weights_history_2d[0, 1]], \n",
    "            c='lime', \n",
    "            s=200, \n",
    "            marker='o', \n",
    "            label='Inicio', \n",
    "            edgecolors='black', \n",
    "            linewidths=2,\n",
    "            zorder=10)\n",
    "ax2.scatter([weights_history_2d[-1, 0]], \n",
    "            [weights_history_2d[-1, 1]], \n",
    "            c='red', \n",
    "            s=200, \n",
    "            marker='*', \n",
    "            label='Final', \n",
    "            edgecolors='black', \n",
    "            linewidths=2,\n",
    "            zorder=10)\n",
    "ax2.set_xlabel(f'w_{top_2_indices[0]}', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel(f'w_{top_2_indices[1]}', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Trajetoria 2D sobre Curvas de Nivel de Loss', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "cbar2 = plt.colorbar(contourf, ax=ax2, label='Loss')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de barras: Importância dos pesos\n",
    "plt.figure(figsize=(14, 6))\n",
    "x_pos = np.arange(num_features)\n",
    "colors = ['red' if i in top_2_indices else 'steelblue' for i in range(num_features)]\n",
    "bars = plt.bar(x_pos, importance_score, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "plt.xlabel('Índice do Peso', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Score de Importância', fontsize=14, fontweight='bold')\n",
    "plt.title('Importância dos Pesos (Score Combinado)', fontsize=18, fontweight='bold')\n",
    "plt.xticks(x_pos, [f'w_{i}' for i in range(num_features)])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (idx, score) in enumerate(zip(x_pos, importance_score)):\n",
    "    if idx in top_2_indices:\n",
    "        plt.text(i, score + 0.005, 'TOP 2', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
